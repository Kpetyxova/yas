import os
import argparse

import evaluate

from utils import read_jsonl_file, write_json_file


chrf = evaluate.load("chrf")
sacrebleu = evaluate.load("sacrebleu")
rouge = evaluate.load("rouge")
accuracy = evaluate.load("accuracy")

def compute_metrics(pairs):
    """
    Compute evaluation metrics for a list of pairs.

    Args:
        pairs (list): A list of pairs, where each pair contains a reference and a hypothesis.

    Returns:
        dict: A dictionary containing the computed metrics.

    """
    metrics = {}
    references_lst = [[pair[0]] for pair in pairs]
    references = [pair[0] for pair in pairs]
    hypotheses = [pair[1] for pair in pairs]

    chrf_score = chrf.compute(predictions=hypotheses, references=references_lst)
    metrics["chrf"] = chrf_score["score"]
    sacrebleu_score = sacrebleu.compute(predictions=hypotheses, references=references_lst)
    metrics["sacrebleu"] = sacrebleu_score["score"]
    rouge_score = rouge.compute(predictions=hypotheses, references=references)
    metrics["rouge1"] = rouge_score["rouge1"] * 100
    metrics["rouge2"] = rouge_score["rouge2"] * 100
    metrics["rougeL"] = rouge_score["rougeL"] * 100
    return metrics

def pg_accuracy(correct_personas, predicted_personas) -> dict:
    """
    Calculates the strict and soft accuracy metrics for predicted personas.

    Parameters:
    correct_personas (list): A list of correct personas.
    predicted_personas (list): A list of predicted personas.

    Returns:
    dict: A dictionary containing the strict and soft accuracy metrics.

    The strict accuracy is calculated as the percentage of correct predictions
    where the predicted persona exactly matches the correct persona.

    The soft accuracy is calculated as the percentage of correct predictions
    where the predicted persona matches the correct persona in terms of presence
    or absence of each persona trait.

    Example usage:
    correct_personas = [[1, 2, 3], [4, 5]]
    predicted_personas = [[1, 2, 3], [4, 5]]
    metrics = pg_accuracy(correct_personas, predicted_personas)
    print(metrics)
    {'strict_accuracy': 100.0, 'soft_accuracy': 100.0}
    """
    metrics = {}
    correct = 0
    for i, cp in enumerate(correct_personas):
        if cp == predicted_personas[i]:
            correct += 1
    strict = correct / len(correct_personas) * 100
    metrics["strict_accuracy"] = strict
    correct_soft = 0
    total_soft = 0
    for k, cp2 in enumerate(correct_personas):
        for j in range(1, len(cp2) + 1):
            if (j in cp2) and (j in predicted_personas[k]):
                correct_soft += 1
            elif (j not in cp2) and (j not in predicted_personas[k]):
                correct_soft += 1
            total_soft += 1
    soft = correct_soft / total_soft * 100
    metrics["soft_accuracy"] = soft
    return metrics

def evaluate_outputs(file_name: str) -> None:
    """
    Evaluate the outputs generated by a model and compute metrics.

    Args:
        file_name (str): The name of the JSONL file containing the outputs.

    Returns:
        None
    """
    data = read_jsonl_file(file_name)
    tag = file_name.replace(".jsonl", "")
    pairs = []
    correct_personas = []
    predicted_personas = []
    for d in data:
        if ("PG" in tag) and ("ZS" not in tag):
            original = d["output"]
            original = original.split("\n", maxsplit=1)[1]
        else:
            original = d["output"]
        pairs.append((original, d["output_generated"]))
    generation_metrics = compute_metrics(pairs)
    compute_metrics(pairs)
    if ("PG" in tag) and ("ZS" not in tag):
        for d in data:
            correct_personas.append(sorted([int(i) for i in d["correct_personas"]]))
            predicted_personas.append(sorted(d["predicted_personas"]))
        grounding_metrics = pg_accuracy(correct_personas, predicted_personas)
        generation_metrics = {**generation_metrics, **grounding_metrics}
    write_json_file(f"{tag}_metrics.json", generation_metrics)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate text generation outputs.")
    parser.add_argument("--filename", type=str, help="The JSONL file to evaluate.")
    args = parser.parse_args()
    evaluate_outputs(args.filename)
